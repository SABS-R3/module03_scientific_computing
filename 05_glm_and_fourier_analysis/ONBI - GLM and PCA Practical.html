<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0058)https://users.fmrib.ox.ac.uk/~saad/ONBI/glm_practical.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="stylesheet" type="text/css" href="./ONBI - GLM and PCA Practical_files/fsl.css">
<script type="text/javascript" src="./ONBI - GLM and PCA Practical_files/showhide.js"></script>
<title>ONBI - GLM and PCA Practical</title>
</head>
<body>
<div id="practical">
<h1 class="centred">ONBI - GLM and PCA Practical</h1>

<hr>

<h2>Practical Overview</h2>
<p>This practical requires Python. Go through the page and execute the listed
  commands in your IDE of choice (you can copy-paste). Don't click on
  the "answers" links until you have thought hard about the
  question. Raise your hand should you need any help.
</p><h3>Contents:</h3>
<dl class="contents">
<dt><a href="https://users.fmrib.ox.ac.uk/~saad/ONBI/glm_practical.html#simpleGLM">General Linar Model</a></dt>
<dd>Fitting the General Linar Model to some data</dd>
<dt><a href="https://users.fmrib.ox.ac.uk/~saad/ONBI/glm_practical.html#PCA">Principal Component Analysis</a></dt>
<dd>Doing PCA on some data</dd>

</dl>
<hr>
<h2><a name="simpleGLM">Simple GLM </a></h2>

<p>Let's start simple. Using Python, generate noisy data <b>y</b> using a
  linear model with one regressor <b>x</b> and an intercept. I.e. <code>y=a+b*x</code> 
</p><pre>  
import numpy as np
import scipy
import matplotlib.pylab as plt
import pandas as pd

x = np.arange(1, 20).reshape(-1, 1)
intercept   = -10
slope       = 2.5
y = intercept + slope*x
y = y + 10*np.random.randn(len(y),1) # add some noise
</pre>
<p>Now plot the data against <b>x</b>:
</p><pre>  
plt.plot(x,y,'o')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>Let's compare fitting a linear model with and without the
  intercept. First, set up two design matrices:
</p><pre>    
M1 = x                                           # w/o intercept
M2 = np.concatenate([np.ones((len(x),1)), x], axis=1) # w/ intercept
</pre>
<p>Look at M1 and M2 if you are not sure what they should look like
  (e.g. type M1 in the Matlab command window and press enter).
</p><p>Now solve the two GLMs:
  </p><pre>    
beta1 = np.linalg.pinv(M1) @ y
beta2 = np.linalg.pinv(M2) @ y
  </pre>  
<p>Look at the values in beta1 and beta2. How should we interpret the
  meaning of these values? 
<span class="clickme" onclick="showIt(&#39;hint0&#39;)">Answer</span> 
  </p><div id="hint0" class="answer" style="display: none"> 
<p>In the above, beta1 contains the estimate of the slope, whereas
  beta2 contains estimates for the slope and the intercept (display
  both beta1 and beta2 and make sure you know which is the slope and
  which is the intercept - check against the design matrices).
</p></div>

<p>Plot the two models against the data:
  </p><pre>    
plt.grid(True)
plt.plot(x,y,'o')
plt.plot(x, M1 @ beta1, 'r-')
plt.plot(x, M2 @ beta2, 'b-')
plt.show()
  </pre>
<p>Note how the red line must go through the
  origin <code>(0,0)</code>, as there is no intercept in the model
  (the intercept is always zero).
</p><p>Note also how in this case the slope is higher when 
  modelling the intercept. If we were to do statistics on this slope
  coefficient (to determine if it were significantly different from
  zero), would we find it more or less significant than if we hadn't
  modelled the intercept?
<span class="clickme" onclick="showIt(&#39;hint1&#39;)">Answer</span> 
  </p><div id="hint1" class="answer" style="display: none"> 
    It can go either way. If the intercept was positive, beta1 would
    have been higher, but model fit 
    poorer, which affects the variance on the estimate of beta1. 
  </div> 
<p>Compare fitting the same <b>M2</b> model (i.e. with a column of
  one's as a regressor), with and without demeaning <b>x</b>. What
  happens to the beta coefficients? How do we interpret the values in
  beta2 after demeaning <b>x</b>? 
<span class="clickme" onclick="showIt(&#39;hint2&#39;)">Answer</span> 
  </p><div id="hint2" class="answer" style="display: none"> 
    Let's first fit the demeaned model and display the regression
    coefficients (the <code>betas</code>):
    <pre>      
M3 = np.concatenate([np.ones((len(x),1)), x-np.mean(x)], axis=1)
beta3 = np.linalg.pinv(M3) @ y
print(beta2)
print(beta3)
    </pre>        
    Notice that beta2(1) is closer to the intercept, and beta3(1) closer to
    the mean of <b>y</b>. The column of one's in the design matrix
    models the intercept if <b>x</b> is not demeaned, but models the mean if
    <b>x</b> is demeaned. Notice that the estimate of the slope does
    not change between the two models, despite changing the
    corresponding regressor!
  </div> 
<hr>
<p>Ok now let's have some fun with some real data. Download the Oxford
  weather data textfile from this
  link: <a href="http://www.fmrib.ox.ac.uk/~saad/ONBI/OxfordWeather.txt">OxfordWeather</a>
  (right-click on the link and save on the disk). 
</p><p>Now load the text file in matlab and arrange the columns of the
  text file into different variables:
</p><pre>  
names = ['year', 'month', 'maxTemp', 'minTemp', 'hoursFrost', 'rain', 'hoursSun']
df = pd.read_csv('OxfordWeather.txt', delim_whitespace=True, header=None, names=names)

meanTemp   = .5*(df.minTemp+df.maxTemp)

</pre>
<p>Plot some of the data to familiarise yourself with it.
</p><pre>
fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3)
decades = (np.round(df.year/10)*10)
ax1.boxplot([df.rain[decades == i] for i in decades.unique()],
        positions=decades.unique(),
        widths=5)
ax1.set_xlabel('decade')
ax1.set_ylabel('rain')

ax2.boxplot([df.rain[df.month==i] for i in df.month.unique()],
        positions=df.month.unique())
ax2.set_xlabel('month')
ax2.set_ylabel('rain')

ax3.plot(df.minTemp, df.maxTemp, '.k')
ax3.set_xlabel('minTemp')
ax3.set_ylabel('maxTemp')

ax4.plot(meanTemp, df.hoursFrost,'.k')
ax4.set_xlabel('meanTemp')
ax4.set_ylabel('hoursFrost')

temps = np.round(meanTemp/3)*3
ax5.boxplot([df.rain[temps==i] for i in temps.unique()],
    positions=temps.unique())
ax5.set_xlabel('meanTemp')
ax5.set_ylabel('rain')

ax6.boxplot([df.hoursSun[df.month==i] for i in df.month.unique()],
    positions=df.month.unique())
ax6.set_xlabel('month')
ax6.set_ylabel('hoursSun')

plt.show()
 
</pre>
<p>You may notice from these plots that there is nothing much to model
  about the rain (at least with a GLM). It simply rains all the
  time... Instead, let's model the rest of the data in various ways:
</p><ul>
  <li>Fit a linear model with x=min and y=max monthly temperatures.
    <span class="clickme" onclick="showIt(&#39;hint3&#39;)">Answer.</span> 
  <div id="hint3" class="answer" style="display: none"> 
    <pre>
# Fit linear model ( y=a*x )
x = df.minTemp.values.reshape(-1,1)
y = df.maxTemp.values.reshape(-1,1)

M = x
beta = np.linalg.pinv(M) @ y

plt.plot(x, y, 'o')
plt.plot(x, M @ beta,'r')
plt.xlabel('minTemp')
plt.ylabel('maxTemp')

# Need a constant? ( y=a*x+b )   [what is b?]
M = np.concatenate([np.ones((len(x),1)), x], axis=1)
beta = np.linalg.pinv(M) @ y
plt.plot(x, M @ beta, 'g')
plt.show()

    </pre>

  </div> 
  </li>
  <li>Fit a quadratic model of the form y=a*x^2+b*x+c with x=month and
  y=hoursSun. 
    <span class="clickme" onclick="showIt(&#39;hint4.1&#39;)">Hint.</span> 
    <div id="hint4.1" class="answer" style="display: none"> 
      A quadratic model is simply a linear model with x and x^2 as
      regressors (on top of the intercept).
    </div>
    <span class="clickme" onclick="showIt(&#39;hint4.2&#39;)">Answer.</span> 
  <div id="hint4.2" class="answer" style="display: none"> 
    <pre>
# Fit quadratic model ( y=a*x^2+b*x+c )
x = df.month.values.reshape(-1,1)
y = df.hoursSun.values.reshape(-1,1)

i = np.argsort(x,axis=0).reshape(-1)  # we have to sort the data according to x if we want a nice plot
x = x[i]
y = y[i]

M = np.concatenate([np.ones_like(x), x, x**2], axis=1)  # this is the cunning bit
beta = np.linalg.pinv(M) @ y
plt.plot(x, y, 'o')
plt.plot(x, M @ beta, 'r')
plt.xlabel('month')
plt.ylabel('hoursSun')
plt.show()

    </pre>
    </div>
  </li>
  <li>Fit an exponential model with x=min temperature and
  y=hoursFrost.
    <span class="clickme" onclick="showIt(&#39;hint5.1&#39;)">Hint.</span> 
    <div id="hint5.1" class="answer" style="display: none">       
      Start by log-transforming the data and notice that you end up
      with a simple GLM.
    </div>
    <span class="clickme" onclick="showIt(&#39;hint5&#39;)">Answer.</span> 
  <div id="hint5" class="answer" style="display: none"> 
    <pre>
# Fit exponential model ( y=a*exp(b*x) )

x = df.minTemp.values.reshape(-1,1)
y = np.log(df.hoursFrost.values.reshape(-1,1)) # trick: in order to fit exp with GLM, use log transform!

# unfortunately, this trick only works for hoursFrost&gt;0. So we need to
# exclude data points where hoursFrost=0:
x = x[df.hoursFrost > 0]
y = y[df.hoursFrost > 0]

i = np.argsort(x, axis=0).reshape(-1) # again, easier to plot if we sort x and y
x = x[i]
y = y[i]

M = np.concatenate([np.ones_like(x), x], axis=1)
beta = np.linalg.pinv(M) @ y

plt.plot(meanTemp, df.hoursFrost, 'o')
plt.plot(x, np.exp(M @ beta), 'r')
plt.xlabel('temp')
plt.ylabel('hoursFrost')
plt.show()

# Note: the exponential model doesn't quite work for this
# data. Below we will try a polynomial model

    </pre>
    <p>Note that when applying a transform to
      the data, there are two issues: (1) the transform is not always
      defined, and (2) the sum of squared error is minimised in the
      transformed domain, so may not be optimal for the actual
      data. The fitted parameters using the GLM can still be used to
      initialise a nonlinear fitting procedure.
  </p></div>  
  </li>
  <li>
    Write a generic function that can fit a polynomial of user-defined
    order (i.e. y=a+b*x+c*x^2+d*x^3+...) and fit this model to the
    hoursFrost data using meanTemp as a regressor. What polynomial
    order gives the best fit without over-fitting?
    <span class="clickme" onclick="showIt(&#39;hint6&#39;)">Answer.</span> 
    <div id="hint6" class="answer" style="display: none">       
      Create a text file called fit_poly.m with the following code:
      <pre>	
def fit_poly(x,y,p):
    """
    [beta,pred,err,sse]=fit_poly(x,y,p)

    p is the degree of the polynomial
    """

    M = np.ones_like(x)
    for i in range(1,p):
        xx = x**i
        xx = xx - np.mean(xx)
        xx = xx / np.linalg.norm(xx)
        M  = np.concatenate([M, xx], axis=1)

    beta = np.linalg.pinv(M) @ y   # regression coefficient

    pred = M @ beta       # data predicted by the model
    err  = y-M @ beta     # error
    sse  = np.sum(err**2) # sum of squared error
    return [beta,pred,err,sse]

      </pre>
      Use the above function on meanTemp/hoursFrost data:
      <pre>	x=meanTemp;


x = meanTemp.values.reshape(-1,1)
y = df.hoursFrost.values.reshape(-1,1)

i = np.argsort(x, axis=0).reshape(-1)
x = x[i]
y = y[i]

plt.plot(x, y, 'o')
plt.xlabel('meanTemp')
plt.ylabel('hoursFrost')

[beta, pred, err, sse] = fit_poly(x, y, 5)
plt.plot(x, pred, 'r', lw=2)
plt.show()

      </pre>
      <p>Qualitatively, a polynomial of order 5 seems to give a good fit
	without over-fitting. Techniques for formally testing model
	order are out of scope here, but we can mention cross-validation
	as an example.
      </p><p>There are many ways of doing cross-validation, the simplest
      being to exclude one or a few data-points, fit the model, and
      test the error on the left-out data points. Repeat the process
      for different left-out data, and look at the average error. This
      method gives very good results in terms of "predictive" power,
      i.e. how good a model is at predicting unseen data.
    </p></div>
  </li>
  <li>
    Try to write down other models that can be fitted with the GLM
    (including via transformation of the data) and use simulated data
    to fit them. 
    <span class="clickme" onclick="showIt(&#39;hint6.1&#39;)">Some examples.</span> 
    <div id="hint6.1" class="answer" style="display: none">   
      Here are some examples. 
      <pre>	
#y = a * np.exp(b*x + c*x**2)  # unknown:a,b,c : log transform the data to get GLM
#y = a * np.cos(x + phi)       # unknown a,phi : use trigonometric equalities to get GLM
#y = 1 / (1 + a*np.exp(b*x))   # unknown a,b   : log-transform
      </pre>
      <p>Let's do the cosine example as it is the most fun:
	</p><pre>	  
x   = np.linspace(0,10,40).reshape(-1,1)
a   = 5     # amplitude parameter
phi = np.pi/4 # phase parameter

y   = a * np.cos(x+phi)            # generate data using cosine model
y   = y + np.random.randn(len(y),1)  # add noise

plt.plot(x, y, 'o')

# the trick:
# a*cos(x+phi) = a*np.cos(x)*np.cos(phi) - a*np.sin(x)*np.sin(phi)
#              = [a*cos(phi)]*(cos(x)) + [a*sin(phi)]*(-sin(x))
# let A=a*cos(phi) and B=a*sin(phi)
# which can be inverted thusly: a=norm([A;B]) and phi=atan2(B/a,A/a)
#
# We now have a linear model for A and B: y = A*cos(x)+B*(-sin(x))


M    = np.concatenate([np.cos(x), -np.sin(x)], axis=1)
beta = np.linalg.pinv(M) @ y

a_hat   = np.linalg.norm(beta)
phi_hat = np.arctan2(beta[1]/a_hat,beta[0]/a_hat)

plt.plot(x, a_hat*np.cos(x+phi_hat), 'r')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

	</pre>
      <p>One might ask if we can fit <code>y=a*cos(w*x+phi)</code>
      for <code>(a,w,phi)</code>, i.e. find the frequency as well as
      the amplitude and phase. We of course can, but this is no longer
      a linear model, and we need to resort to other means (such as
      nonlinear fitting).
      
    </p></div>
    
  </li>
</ul>


<hr>
<hr>
<h2><a name="simpleEig">Eigenvalues/Eigenvectors of data clouds</a></h2>
<p>Here we will switch gears and turn our attention to eigenvalue
  decompositions. We will use eigenvalues in the context of data
  dimensionality reduction. 
</p><p>First, let's generate a 2D data cloud. The following code generates
  data that follows a Gaussian distribution with a covariance matrix
  that has a 60 degree rotation with respect to the x-axis. Read the
  code carefully and try to understand all the different steps. 
</p><pre>  
# mean position
mu = np.array([5,2])

# angle in degrees
ang = 60;

# rotation matrix
ang_rad = np.radians(ang)
R = np.array([
        [np.cos(ang_rad), -np.sin(ang_rad)],
        [np.sin(ang_rad),  np.cos(ang_rad)],
])

# rotated covariance matrix
sig = R @ np.array([[1.3, 0], [0, .1]]) @ R.transpose()

# create points using multivariate gaussian random
y = np.random.multivariate_normal(mu, sig, 100)

# plot the data
plt.plot(y[: , 0], y[: , 1], 'b.')
plt.xlabel('x')
plt.ylabel('y')

</pre>
<p>Now keep the previous plot open, demean the data, and plot the
  demeaned data on the same plot.
</p><pre>  
ydemean = y - np.mean(y,axis=0)
plt.plot(ydemean[:,0], ydemean[:,1], 'k.')
plt.show()
</pre>
<p>Next, calculate the eigenvalue decomposition of the covariance
  matrix and plot the eigenvectors scaled by the square-root of their eigenvalues.
</p><pre>  
d,v = np.linalg.eig(np.cov(y.transpose()))
v = np.abs(v)
d = np.sqrt(np.abs(d))


# Plot the two eigenvectors
plt.plot(ydemean[:,0], ydemean[:,1], 'k.')
plt.arrow(0,0,d[0]*v[0,0],d[0]*v[1,0],color='b',width=0.05)   # minor eigenvector
plt.arrow(0,0,d[1]*v[0,1],d[1]*v[1,1],color='r',width=0.05)   # major eigenvector
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>It should be clear that the major eigenvector (in red) is aligned
  with the direction of most "variance" in the data, and that the
  minor eigenvector is orthogonal to the major one, and is aligned 
  with the direction of least variance. This is true because
  we have generated data using a Gaussian distribution.
</p><p>Now project the data onto the first eigenvector and plot the
  projected data.
</p><pre>  
ydemean = Y - np.mean(Y, axis=0)
yy = (ydemean @ v[:,1].reshape(-1,1))*v[:,1].reshape(1,-1)
plt.plot(yy[:,0],yy[:,1],'g.');
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>Now we do the same thing, but this time on a data set that does not
  have an interesting direction of maximum variance, but instead has
  information that is cast along a "radial" dimension.
</p><p>Start by generating the doughnut data set (make sure you understand
  what each line is doing in the following code):
</p><pre>  
t = 2*np.pi*np.random.rand(1000,1) # random angle
r = .5+np.random.rand(1000,1)      # random radius between .5 and 1.5
x = r*np.sin(t)
y = r*np.cos(t);

plt.clf()
plt.plot(x,y,'o')
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>Now do the PCA on this data by calculating the eigenvalues of the
  covariance matrix:
</p><pre>  
Y = np.concatenate([x, y], axis=1)
d,v = np.linalg.eig(np.cov(Y.transpose()))
v = np.abs(v)
d = np.sqrt(np.abs(d))

plt.plot(x,y,'o')
plt.arrow(0,0,d[1]*v[0,1],d[1]*v[1,1],color='r',width=0.05)
plt.arrow(0,0,d[0]*v[0,0],d[0]*v[1,0],color='b',width=0.05)
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>The resuting directions, as you can see, do not indicate any
  particularly useful feature of the data. In fact, they are
  completely random (driven by the noise, since the model is
  completely symmetric). 
</p><p>Furthermore, if we project the data onto
  the mean eigenvector, we loose all information on the doughnut:
</p><pre>    
ydemean = Y - np.mean(Y, axis=0)
yy = (ydemean @ v[:,1].reshape(-1,1))*v[:,1].reshape(1,-1)
plt.plot(yy[:,0],yy[:,1],'g.');
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>Now one last data-cloud example. Consider the data generated with
  the following bit of code:
</p><pre>  
y1 = np.random.multivariate_normal(np.array([0,0]),
                                   np.array([[1,0],[0,20]]),
                                   1000)
y2 = np.random.multivariate_normal(np.array([5,0]),
                                    np.array([[1,0],[0,20]]),
                                    1000);
y = np.concatenate([y1,y2],axis=0)
y = y - np.mean(y, axis=0)    # demeand the data to keep things simple

plt.clf()
plt.plot(y[:,0],y[:,1],'k.')
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>As you can see, the data comes in two interesting
  clusters. However, the direction of maximum variance is orthogonal
  to the direction that separates the two clusters. Let's see how this
  affects the results of a data reduction that uses PCA.
</p><p>First calculate the eigenspectrum of the covariance matrix and plot
  the major and minor eigenvectors:
</p><pre>  
d,v = np.linalg.eig(np.cov(y.transpose()))
v = np.abs(v)
d = np.sqrt(np.abs(d))
plt.plot(y[:,0],y[:,1],'k.')
plt.arrow(0,0,d[1]*v[0,1],d[1]*v[1,1],color='r',width=0.05); # major eigenvector
plt.arrow(0,0,d[0]*v[0,0],d[0]*v[1,0],color='b',width=0.05); # minor eigenvector
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>
<p>Now project the data onto the first eigenvector. You can see that
  the cluster information is completely lost. PCA is not a good way to
  cleanup this data.
</p><pre>  
yy = (y @ v[:,1].reshape(-1,1)) @ v[:,1].reshape(1,-1);
plt.plot(yy[:,0],yy[:,1],'g.')
plt.axis('equal')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
</pre>

<hr>

<h2><a name="PCA">PCA of the world</a></h2>
<p>Let us finish with a fun example, PCA of the world map. I've chosen the world map
  because, although the data is multi-dimensional, which precludes
  from simple data-cloud style visualisation, information in the data can still be
  visualised as a matrix.
</p><p>First, download the data file
  from <a href="http://www.fmrib.ox.ac.uk/~saad/ONBI/world-map.gif">here</a>.
</p><p>Now open the data in Matlab and visualise it
  with <code>imagesc</code>.
</p><pre>  
import skimage as ski
import skimage.io

im = ski.io.imread('world-map.gif').astype('float64')  # read image
im = np.sum(im, axis=2)                  # sum across the three colour channels
plt.imshow(im)                           # display the image
plt.gray()
plt.show()
</pre>
<p>Start by calculating the eigenspectrum of the data covariance
  matrix:
</p><pre>  
d,v = np.linalg.eig(np.cov(im.transpose()))
</pre>
<p>Now we are going to reduce the dimensionality of the data by
  reconstructing the data using a subset of the eigenvectors. To this
  end, we need to first demean the rows of the data matrix:
</p><pre>  
x = im
m = np.mean(x, axis=1)

for i in range(x.shape[0]):
    x[i,:] = x[i,:] - m[i]

</pre>
<p>Next, reconstruct the data using only the top <code>p</code>
  eigenvectors. Look at what happens with and without adding the mean
  back to the data. Play with different values of <code>p</code>. When
  do we start seeing Great Britain? 
  </p><pre> 
p = 4

# reconstruct the data with top p eigenvectors
y = (x @ v[:,:p]) @ v[:,:p].transpose()

# plotting
fig, (ax1, ax2, ax3) = plt.subplots(3, 1)
ax1.imshow(im)
ax2.imshow(np.abs(y))

# add mean
for i in range(x.shape[0]):
    y[i,:] = y[i,:] + m[i]
ax3.imshow(np.abs(y))
plt.show()
  </pre>

  <p> Same as above but with <a href="http://www.fmrib.ox.ac.uk/~saad/ONBI/Queen_Victoria_by_Bassano.jpg">Queen Victoria</a>. When do you start to see
    her moustache?
</p><pre>  
im = ski.io.imread('Queen_Victoria_by_Bassano.jpg').astype('float64')
im = np.sum(im, axis=2)

# reconstruct the data with top p eigenvectors
d,v = np.linalg.eig(np.cov(im.transpose()))
idx = d.argsort()[::-1]
v = v[:,idx]
p = 4
x = im
m = np.mean(x, axis=1)
for i in range(x.shape[0]):
    x[i,:] = x[i,:] - m[i]
y = (x @ v[:,:p]) @ v[:,:p].transpose()

# plotting
fig, (ax1, ax2, ax3) = plt.subplots(3, 1)
ax1.imshow(im)
ax2.imshow(y)

# add mean
for i in range(x.shape[0]):
    y[i,:] = y[i,:] + m[i]
ax3.imshow(y)
plt.show()
</pre>

<hr>
<hr>

<p class="centred">The End.</p>

<hr>
<hr>
<small>Created by Saad Jbabdi (<a href="mailto:saad@fmrib.ox.ac.uk">saad@fmrib.ox.ac.uk</a>)</small>
</div>



</body></html>
